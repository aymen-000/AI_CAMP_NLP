{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Experimenting with HuggingFace - Text Generation\n",
    "**We will explore text generation using a GPT-2 model, which was trained to predict next words on 40GB of Internet text data.**\n",
    "**In this notebook, we will explore different decoding methods like Beam search, Top-K sampling, and Top-P sampling, demonstrating their performance along the way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:03:12.550011Z",
     "iopub.status.busy": "2024-12-04T09:03:12.549350Z",
     "iopub.status.idle": "2024-12-04T09:03:12.578281Z",
     "shell.execute_reply": "2024-12-04T09:03:12.577307Z",
     "shell.execute_reply.started": "2024-12-04T09:03:12.549972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"SEED\" : 34 ,\n",
    "    \"MAX_LEN\" : 70 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A language model is a machine learning model that can look at part of a sentence and predict the next word/sequence of words. Much like the autofill features , GPT-2 is capable of next word prediction on a much larger and more sophisticated scale. For reference, the smallest available GPT-2 has 117 million parameters, whereas the largest one (invisible to the public) has over 1.5 billion parameters. The largest one available for public use is half the size of their main GPT-2 mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T22:28:51.247884Z",
     "iopub.status.busy": "2024-12-03T22:28:51.247648Z",
     "iopub.status.idle": "2024-12-03T22:29:17.105339Z",
     "shell.execute_reply": "2024-12-03T22:29:17.104270Z",
     "shell.execute_reply.started": "2024-12-03T22:28:51.247861Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.0.post2.tar.gz (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.4.0)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.0.post2-cp310-cp310-linux_x86_64.whl size=183279716 sha256=c1021a1c990422f49f554a42308c4dbfb14501d6c51bb6b595c3784ef8b86acf\n",
      "  Stored in directory: /root/.cache/pip/wheels/bf/e3/ed/5e845387d52f2debd1bafb847bf3d774d3f0a3c8e31b1dc948\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.0 flash-attn-2.7.0.post2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:03:12.580137Z",
     "iopub.status.busy": "2024-12-04T09:03:12.579850Z",
     "iopub.status.idle": "2024-12-04T09:03:36.589186Z",
     "shell.execute_reply": "2024-12-04T09:03:36.588223Z",
     "shell.execute_reply.started": "2024-12-04T09:03:12.580111Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b192fece4c1b46b2868084f9c259de80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80146c9ecc4e434c9083b03a82ef212f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a726430b75403e912ed445059dd414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cf39d7599d4a00b3dcea26fbf6e838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa0b9437d61497ea16741dd13c08539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede5caedf9c3478e80d578d78c0f519f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c3dd7739304da782b8f0b8d3a739b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architacture : \n",
      " GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ") \n"
     ]
    }
   ],
   "source": [
    "import torch # import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # from transformers \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # set device based on your machine\n",
    "\n",
    "model_name = \"gpt2\" # load gpt-2 model \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # load tokenizer \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) \n",
    "\n",
    "print(f\"Model architacture : \\n {model} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wow, we just imported a deep learning model with more than 774 million parameters in just a couple lines of code with HuggingFace , that is the power of HuggingFace !!**\n",
    "**Now let’s generate some text! Although Transformers provides a generate() func‐\n",
    "tion for autoregressive models like GPT-2, we’ll implement this decoding method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Different Decoding Methods\n",
    "### 2.1 Greedy Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:03:36.591873Z",
     "iopub.status.busy": "2024-12-04T09:03:36.591123Z",
     "iopub.status.idle": "2024-12-04T09:03:37.034662Z",
     "shell.execute_reply": "2024-12-04T09:03:37.033632Z",
     "shell.execute_reply.started": "2024-12-04T09:03:36.591830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "Input: Transformers are the\n",
      "Choice 1:  most (9.76%)\n",
      "Choice 2:  same (2.94%)\n",
      "Choice 3:  only (2.87%)\n",
      "Choice 4:  best (2.38%)\n",
      "Choice 5:  first (1.77%)\n",
      "\n",
      "Step 2:\n",
      "Input: Transformers are the first\n",
      "Choice 1:  to (12.16%)\n",
      "Choice 2:  of (4.14%)\n",
      "Choice 3:  and (3.95%)\n",
      "Choice 4:  class (2.70%)\n",
      "Choice 5:  generation (1.88%)\n",
      "\n",
      "Step 3:\n",
      "Input: Transformers are the first generation\n",
      "Choice 1:  of (57.15%)\n",
      "Choice 2:  to (1.53%)\n",
      "Choice 3: , (1.07%)\n",
      "Choice 4:  in (0.93%)\n",
      "Choice 5: . (0.68%)\n",
      "\n",
      "Step 4:\n",
      "Input: Transformers are the first generation.\n",
      "Choice 1: \n",
      " (21.86%)\n",
      "Choice 2:  They (9.65%)\n",
      "Choice 3:  The (9.13%)\n",
      "Choice 4:  This (2.14%)\n",
      "Choice 5: \n",
      "\n",
      " (2.13%)\n",
      "\n",
      "Step 5:\n",
      "Input: Transformers are the first generation.\n",
      "\n",
      "\n",
      "Choice 1: \n",
      " (99.96%)\n",
      "Choice 2: . (0.01%)\n",
      "Choice 3:  ( (0.00%)\n",
      "Choice 4: \n",
      "\n",
      " (0.00%)\n",
      "Choice 5: , (0.00%)\n",
      "\n",
      "Step 6:\n",
      "Input: Transformers are the first generation.\n",
      "\n",
      ",\n",
      "Choice 1:  the (5.36%)\n",
      "Choice 2:  and (4.23%)\n",
      "Choice 3:  which (2.85%)\n",
      "Choice 4:  a (2.17%)\n",
      "Choice 5: \n",
      " (2.09%)\n",
      "\n",
      "Step 7:\n",
      "Input: Transformers are the first generation.\n",
      "\n",
      ",\n",
      "\n",
      "Choice 1: \n",
      " (90.90%)\n",
      "Choice 2: The (0.81%)\n",
      "Choice 3: \n",
      "\n",
      " (0.46%)\n",
      "Choice 4: \" (0.42%)\n",
      "Choice 5: This (0.19%)\n",
      "\n",
      "Step 8:\n",
      "Input: Transformers are the first generation.\n",
      "\n",
      ",\n",
      "This\n",
      "Choice 1:  is (22.42%)\n",
      "Choice 2:  means (2.18%)\n",
      "Choice 3:  mod (1.79%)\n",
      "Choice 4:  will (1.73%)\n",
      "Choice 5:  was (1.38%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)  # return tensor\n",
    "iters = []  # To store the step-by-step generation details\n",
    "\n",
    "n_steps = 8\n",
    "choices_per_step = 5  # Define our k variable (top-k sampling)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(n_steps):\n",
    "        iter = dict()\n",
    "        iter[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "\n",
    "        # Model output\n",
    "        output = model(input_ids=input_ids)\n",
    "\n",
    "        # Select the last token logits for the first batch\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_prob = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Sort the outputs from the highest probs to the lowest\n",
    "        sorted_ids = torch.argsort(next_token_prob, dim=-1, descending=True)\n",
    "\n",
    "        # Get k tokens with the highest probs\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]  # Get token id\n",
    "            token_prob = next_token_prob[token_id].cpu().item()  # Get prob of the target token\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iter[f\"Choice {choice_idx + 1}\"] = token_choice\n",
    "\n",
    "        # Update input_ids with the highest-probability token\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, choice_idx, None]], dim=-1)\n",
    "        \n",
    "        # Store this iteration's details\n",
    "        iters.append(iter)\n",
    "\n",
    "# Example: Display the results\n",
    "for step_idx, iteration in enumerate(iters):\n",
    "    print(f\"Step {step_idx + 1}:\")\n",
    "    for key, value in iteration.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:03:49.774927Z",
     "iopub.status.busy": "2024-12-04T09:03:49.774585Z",
     "iopub.status.idle": "2024-12-04T09:03:49.787127Z",
     "shell.execute_reply": "2024-12-04T09:03:49.786235Z",
     "shell.execute_reply.started": "2024-12-04T09:03:49.774897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (9.76%)</td>\n",
       "      <td>same (2.94%)</td>\n",
       "      <td>only (2.87%)</td>\n",
       "      <td>best (2.38%)</td>\n",
       "      <td>first (1.77%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the first</td>\n",
       "      <td>to (12.16%)</td>\n",
       "      <td>of (4.14%)</td>\n",
       "      <td>and (3.95%)</td>\n",
       "      <td>class (2.70%)</td>\n",
       "      <td>generation (1.88%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the first generation</td>\n",
       "      <td>of (57.15%)</td>\n",
       "      <td>to (1.53%)</td>\n",
       "      <td>, (1.07%)</td>\n",
       "      <td>in (0.93%)</td>\n",
       "      <td>. (0.68%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the first generation.</td>\n",
       "      <td>\\n (21.86%)</td>\n",
       "      <td>They (9.65%)</td>\n",
       "      <td>The (9.13%)</td>\n",
       "      <td>This (2.14%)</td>\n",
       "      <td>\\n\\n (2.13%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the first generation.\\n\\n</td>\n",
       "      <td>\\n (99.96%)</td>\n",
       "      <td>. (0.01%)</td>\n",
       "      <td>( (0.00%)</td>\n",
       "      <td>\\n\\n (0.00%)</td>\n",
       "      <td>, (0.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the first generation.\\n\\n,</td>\n",
       "      <td>the (5.36%)</td>\n",
       "      <td>and (4.23%)</td>\n",
       "      <td>which (2.85%)</td>\n",
       "      <td>a (2.17%)</td>\n",
       "      <td>\\n (2.09%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the first generation.\\n\\n,\\n</td>\n",
       "      <td>\\n (90.90%)</td>\n",
       "      <td>The (0.81%)</td>\n",
       "      <td>\\n\\n (0.46%)</td>\n",
       "      <td>\" (0.42%)</td>\n",
       "      <td>This (0.19%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the first generation.\\n\\n,\\nThis</td>\n",
       "      <td>is (22.42%)</td>\n",
       "      <td>means (2.18%)</td>\n",
       "      <td>mod (1.79%)</td>\n",
       "      <td>will (1.73%)</td>\n",
       "      <td>was (1.38%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input       Choice 1  \\\n",
       "0                               Transformers are the   most (9.76%)   \n",
       "1                         Transformers are the first    to (12.16%)   \n",
       "2              Transformers are the first generation    of (57.15%)   \n",
       "3             Transformers are the first generation.    \\n (21.86%)   \n",
       "4         Transformers are the first generation.\\n\\n    \\n (99.96%)   \n",
       "5        Transformers are the first generation.\\n\\n,    the (5.36%)   \n",
       "6      Transformers are the first generation.\\n\\n,\\n    \\n (90.90%)   \n",
       "7  Transformers are the first generation.\\n\\n,\\nThis    is (22.42%)   \n",
       "\n",
       "         Choice 2        Choice 3        Choice 4             Choice 5  \n",
       "0    same (2.94%)    only (2.87%)    best (2.38%)        first (1.77%)  \n",
       "1      of (4.14%)     and (3.95%)   class (2.70%)   generation (1.88%)  \n",
       "2      to (1.53%)       , (1.07%)      in (0.93%)            . (0.68%)  \n",
       "3    They (9.65%)     The (9.13%)    This (2.14%)         \\n\\n (2.13%)  \n",
       "4       . (0.01%)       ( (0.00%)    \\n\\n (0.00%)            , (0.00%)  \n",
       "5     and (4.23%)   which (2.85%)       a (2.17%)           \\n (2.09%)  \n",
       "6     The (0.81%)    \\n\\n (0.46%)       \" (0.42%)         This (0.19%)  \n",
       "7   means (2.18%)     mod (1.79%)    will (1.73%)          was (1.38%)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put results in dataframe\n",
    "import pandas as pd\n",
    "pd.DataFrame(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:03:51.740151Z",
     "iopub.status.busy": "2024-12-04T09:03:51.739685Z",
     "iopub.status.idle": "2024-12-04T09:03:53.526868Z",
     "shell.execute_reply": "2024-12-04T09:03:53.525777Z",
     "shell.execute_reply.started": "2024-12-04T09:03:51.740116Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length,\n",
    "do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:03:55.765470Z",
     "iopub.status.busy": "2024-12-04T09:03:55.764428Z",
     "iopub.status.idle": "2024-12-04T09:03:56.883552Z",
     "shell.execute_reply": "2024-12-04T09:03:56.882455Z",
     "shell.execute_reply.started": "2024-12-04T09:03:55.765409Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, San Diego, and the University of California, Santa Cruz, found that the unicorns were able to communicate with each other in a way that was similar to that of human speech.\n",
      "\n",
      "\n",
      "\"The unicorns were able to communicate with each other in a way that was similar to that of human speech,\" said study co-lead author Dr. David J.\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
    "do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Top-k Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:03:59.740100Z",
     "iopub.status.busy": "2024-12-04T09:03:59.739414Z",
     "iopub.status.idle": "2024-12-04T09:04:00.547004Z",
     "shell.execute_reply": "2024-12-04T09:04:00.546004Z",
     "shell.execute_reply.started": "2024-12-04T09:03:59.740062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The group's findings were published in the Dec. 1 issue of Nature Communications, among other things.\n",
      "\n",
      "\n",
      "The researchers describe their findings in a video released this morning by the Smithsonian Park Zoo and help researchers prepare for the next major study.\n",
      "\n",
      "\n",
      "\"Familiarity with the spoken-a language does make it easy and easy to learn other languages,\" said senior author Tom M. Hovann,\n"
     ]
    }
   ],
   "source": [
    "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
    "top_k=50)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Top-P sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:04:02.605020Z",
     "iopub.status.busy": "2024-12-04T09:04:02.604625Z",
     "iopub.status.idle": "2024-12-04T09:04:03.427047Z",
     "shell.execute_reply": "2024-12-04T09:04:03.426132Z",
     "shell.execute_reply.started": "2024-12-04T09:04:02.604984Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, who had been studying the unicorn population and then studying their behaviour, found that the unicorns could recognize and interact with an unknown type of animal as well.\n",
      "\n",
      "'The finding is the first evidence that wild unicorns have a language. They may even have an ancestral language,' explained Professor Jochen Weich, a researcher in the Department of Natural Science and Technology at the University of\n"
     ]
    }
   ],
   "source": [
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
    "top_p=0.90)\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.BLEU Metrice\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/1*Vm5DgYvNfl6hrqpVD3n7OA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:04:21.772824Z",
     "iopub.status.busy": "2024-12-04T09:04:21.771919Z",
     "iopub.status.idle": "2024-12-04T09:04:32.747540Z",
     "shell.execute_reply": "2024-12-04T09:04:32.746541Z",
     "shell.execute_reply.started": "2024-12-04T09:04:21.772787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:05:23.980001Z",
     "iopub.status.busy": "2024-12-04T09:05:23.979608Z",
     "iopub.status.idle": "2024-12-04T09:05:34.054593Z",
     "shell.execute_reply": "2024-12-04T09:05:34.053433Z",
     "shell.execute_reply.started": "2024-12-04T09:05:23.979967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-3.0.0 sacrebleu-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:05:05.036981Z",
     "iopub.status.busy": "2024-12-04T09:05:05.036084Z",
     "iopub.status.idle": "2024-12-04T09:05:16.687444Z",
     "shell.execute_reply": "2024-12-04T09:05:16.686327Z",
     "shell.execute_reply.started": "2024-12-04T09:05:05.036940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=6247ca3381e4b78a5bc36f81b7fd9fe0690aa2ebe904da7f4bf3fb2c425260d3\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:05:34.056825Z",
     "iopub.status.busy": "2024-12-04T09:05:34.056449Z",
     "iopub.status.idle": "2024-12-04T09:05:34.426601Z",
     "shell.execute_reply": "2024-12-04T09:05:34.425851Z",
     "shell.execute_reply.started": "2024-12-04T09:05:34.056791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from evaluate import load  # Modern API for metrics\n",
    "\n",
    "# Load the SacreBLEU metric\n",
    "bleu_metric = load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:05:37.491395Z",
     "iopub.status.busy": "2024-12-04T09:05:37.490321Z",
     "iopub.status.idle": "2024-12-04T09:05:37.548972Z",
     "shell.execute_reply": "2024-12-04T09:05:37.547890Z",
     "shell.execute_reply.started": "2024-12-04T09:05:37.491354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>57.893007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[5, 3, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[5, 4, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[100.0, 75.0, 66.67, 50.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Value\n",
       "score                        57.893007\n",
       "counts                    [5, 3, 2, 1]\n",
       "totals                    [5, 4, 3, 2]\n",
       "precisions  [100.0, 75.0, 66.67, 50.0]\n",
       "bp                            0.818731\n",
       "sys_len                              5\n",
       "ref_len                              6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "prediction=\"the cat is on mat\" \n",
    "reference=[\"the cat is on the mat\"]\n",
    "\n",
    "bleu_metric.add(prediction = prediction, reference = reference)\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:05:41.730335Z",
     "iopub.status.busy": "2024-12-04T09:05:41.729935Z",
     "iopub.status.idle": "2024-12-04T09:05:42.576586Z",
     "shell.execute_reply": "2024-12-04T09:05:42.575715Z",
     "shell.execute_reply.started": "2024-12-04T09:05:41.730292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdaebb41cac04a9fb16604559b244c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge_metric = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:05:45.050152Z",
     "iopub.status.busy": "2024-12-04T09:05:45.049746Z",
     "iopub.status.idle": "2024-12-04T09:05:45.230236Z",
     "shell.execute_reply": "2024-12-04T09:05:45.229105Z",
     "shell.execute_reply.started": "2024-12-04T09:05:45.050116Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rouge1    rouge2    rougeL  rougeLsum\n",
       "0  0.909091  0.666667  0.909091   0.909091"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction=\"the cat is on mat\" \n",
    "reference=[\"the cat is on the mat\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_metric.add(prediction=prediction, reference=reference[0])\n",
    "score = rouge_metric.compute()\n",
    "rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n",
    "records.append(rouge_dict)\n",
    "pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
