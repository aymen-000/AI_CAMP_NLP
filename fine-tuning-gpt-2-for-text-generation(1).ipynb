{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Experimenting with HuggingFace - Text Generation\n**We will explore text generation using a GPT-2 model, which was trained to predict next words on 40GB of Internet text data.**\n**In this notebook, we will explore different decoding methods like Beam search, Top-K sampling, and Top-P sampling, demonstrating their performance along the way**","metadata":{}},{"cell_type":"code","source":"config = {\n    \"SEED\" : 34 ,\n    \"MAX_LEN\" : 70 \n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:03:12.549350Z","iopub.execute_input":"2024-12-04T09:03:12.550011Z","iopub.status.idle":"2024-12-04T09:03:12.578281Z","shell.execute_reply.started":"2024-12-04T09:03:12.549972Z","shell.execute_reply":"2024-12-04T09:03:12.577307Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**A language model is a machine learning model that can look at part of a sentence and predict the next word/sequence of words. Much like the autofill features , GPT-2 is capable of next word prediction on a much larger and more sophisticated scale. For reference, the smallest available GPT-2 has 117 million parameters, whereas the largest one (invisible to the public) has over 1.5 billion parameters. The largest one available for public use is half the size of their main GPT-2 mode**","metadata":{}},{"cell_type":"code","source":"!pip install -U flash-attn --no-build-isolation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T22:28:51.247648Z","iopub.execute_input":"2024-12-03T22:28:51.247884Z","iopub.status.idle":"2024-12-03T22:29:17.105339Z","shell.execute_reply.started":"2024-12-03T22:28:51.247861Z","shell.execute_reply":"2024-12-03T22:29:17.104270Z"}},"outputs":[{"name":"stdout","text":"Collecting flash-attn\n  Downloading flash_attn-2.7.0.post2.tar.gz (2.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.4.0)\nCollecting einops (from flash-attn)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.0.post2-cp310-cp310-linux_x86_64.whl size=183279716 sha256=c1021a1c990422f49f554a42308c4dbfb14501d6c51bb6b595c3784ef8b86acf\n  Stored in directory: /root/.cache/pip/wheels/bf/e3/ed/5e845387d52f2debd1bafb847bf3d774d3f0a3c8e31b1dc948\nSuccessfully built flash-attn\nInstalling collected packages: einops, flash-attn\nSuccessfully installed einops-0.8.0 flash-attn-2.7.0.post2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch # import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM # from transformers \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" # set device based on your machine\n\nmodel_name = \"gpt2\" # load gpt-2 model \n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name) # load tokenizer \nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device) \n\nprint(f\"Model architacture : \\n {model} \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:03:12.579850Z","iopub.execute_input":"2024-12-04T09:03:12.580137Z","iopub.status.idle":"2024-12-04T09:03:36.589186Z","shell.execute_reply.started":"2024-12-04T09:03:12.580111Z","shell.execute_reply":"2024-12-04T09:03:36.588223Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b192fece4c1b46b2868084f9c259de80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80146c9ecc4e434c9083b03a82ef212f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a726430b75403e912ed445059dd414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54cf39d7599d4a00b3dcea26fbf6e838"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaa0b9437d61497ea16741dd13c08539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ede5caedf9c3478e80d578d78c0f519f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c3dd7739304da782b8f0b8d3a739b6"}},"metadata":{}},{"name":"stdout","text":"Model architacture : \n GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n) \n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Wow, we just imported a deep learning model with more than 774 million parameters in just a couple lines of code with HuggingFace , that is the power of HuggingFace !!**\n**Now let’s generate some text! Although Transformers provides a generate() func‐\ntion for autoregressive models like GPT-2, we’ll implement this decoding method**","metadata":{}},{"cell_type":"markdown","source":"## 2. Different Decoding Methods\n### 2.1 Greedy Search Decoding","metadata":{}},{"cell_type":"code","source":"import torch\n\ninput_txt = \"Transformers are the\"\n\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)  # return tensor\niters = []  # To store the step-by-step generation details\n\nn_steps = 8\nchoices_per_step = 5  # Define our k variable (top-k sampling)\n\nwith torch.no_grad():\n    for step in range(n_steps):\n        iter = dict()\n        iter[\"Input\"] = tokenizer.decode(input_ids[0])\n\n        # Model output\n        output = model(input_ids=input_ids)\n\n        # Select the last token logits for the first batch\n        next_token_logits = output.logits[0, -1, :]\n        next_token_prob = torch.softmax(next_token_logits, dim=-1)\n\n        # Sort the outputs from the highest probs to the lowest\n        sorted_ids = torch.argsort(next_token_prob, dim=-1, descending=True)\n\n        # Get k tokens with the highest probs\n        for choice_idx in range(choices_per_step):\n            token_id = sorted_ids[choice_idx]  # Get token id\n            token_prob = next_token_prob[token_id].cpu().item()  # Get prob of the target token\n            token_choice = (\n                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n            )\n            iter[f\"Choice {choice_idx + 1}\"] = token_choice\n\n        # Update input_ids with the highest-probability token\n        input_ids = torch.cat([input_ids, sorted_ids[None, choice_idx, None]], dim=-1)\n        \n        # Store this iteration's details\n        iters.append(iter)\n\n# Example: Display the results\nfor step_idx, iteration in enumerate(iters):\n    print(f\"Step {step_idx + 1}:\")\n    for key, value in iteration.items():\n        print(f\"{key}: {value}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:03:36.591123Z","iopub.execute_input":"2024-12-04T09:03:36.591873Z","iopub.status.idle":"2024-12-04T09:03:37.034662Z","shell.execute_reply.started":"2024-12-04T09:03:36.591830Z","shell.execute_reply":"2024-12-04T09:03:37.033632Z"}},"outputs":[{"name":"stdout","text":"Step 1:\nInput: Transformers are the\nChoice 1:  most (9.76%)\nChoice 2:  same (2.94%)\nChoice 3:  only (2.87%)\nChoice 4:  best (2.38%)\nChoice 5:  first (1.77%)\n\nStep 2:\nInput: Transformers are the first\nChoice 1:  to (12.16%)\nChoice 2:  of (4.14%)\nChoice 3:  and (3.95%)\nChoice 4:  class (2.70%)\nChoice 5:  generation (1.88%)\n\nStep 3:\nInput: Transformers are the first generation\nChoice 1:  of (57.15%)\nChoice 2:  to (1.53%)\nChoice 3: , (1.07%)\nChoice 4:  in (0.93%)\nChoice 5: . (0.68%)\n\nStep 4:\nInput: Transformers are the first generation.\nChoice 1: \n (21.86%)\nChoice 2:  They (9.65%)\nChoice 3:  The (9.13%)\nChoice 4:  This (2.14%)\nChoice 5: \n\n (2.13%)\n\nStep 5:\nInput: Transformers are the first generation.\n\n\nChoice 1: \n (99.96%)\nChoice 2: . (0.01%)\nChoice 3:  ( (0.00%)\nChoice 4: \n\n (0.00%)\nChoice 5: , (0.00%)\n\nStep 6:\nInput: Transformers are the first generation.\n\n,\nChoice 1:  the (5.36%)\nChoice 2:  and (4.23%)\nChoice 3:  which (2.85%)\nChoice 4:  a (2.17%)\nChoice 5: \n (2.09%)\n\nStep 7:\nInput: Transformers are the first generation.\n\n,\n\nChoice 1: \n (90.90%)\nChoice 2: The (0.81%)\nChoice 3: \n\n (0.46%)\nChoice 4: \" (0.42%)\nChoice 5: This (0.19%)\n\nStep 8:\nInput: Transformers are the first generation.\n\n,\nThis\nChoice 1:  is (22.42%)\nChoice 2:  means (2.18%)\nChoice 3:  mod (1.79%)\nChoice 4:  will (1.73%)\nChoice 5:  was (1.38%)\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# put results in dataframe\nimport pandas as pd\npd.DataFrame(iters)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:03:49.774585Z","iopub.execute_input":"2024-12-04T09:03:49.774927Z","iopub.status.idle":"2024-12-04T09:03:49.787127Z","shell.execute_reply.started":"2024-12-04T09:03:49.774897Z","shell.execute_reply":"2024-12-04T09:03:49.786235Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                               Input       Choice 1  \\\n0                               Transformers are the   most (9.76%)   \n1                         Transformers are the first    to (12.16%)   \n2              Transformers are the first generation    of (57.15%)   \n3             Transformers are the first generation.    \\n (21.86%)   \n4         Transformers are the first generation.\\n\\n    \\n (99.96%)   \n5        Transformers are the first generation.\\n\\n,    the (5.36%)   \n6      Transformers are the first generation.\\n\\n,\\n    \\n (90.90%)   \n7  Transformers are the first generation.\\n\\n,\\nThis    is (22.42%)   \n\n         Choice 2        Choice 3        Choice 4             Choice 5  \n0    same (2.94%)    only (2.87%)    best (2.38%)        first (1.77%)  \n1      of (4.14%)     and (3.95%)   class (2.70%)   generation (1.88%)  \n2      to (1.53%)       , (1.07%)      in (0.93%)            . (0.68%)  \n3    They (9.65%)     The (9.13%)    This (2.14%)         \\n\\n (2.13%)  \n4       . (0.01%)       ( (0.00%)    \\n\\n (0.00%)            , (0.00%)  \n5     and (4.23%)   which (2.85%)       a (2.17%)           \\n (2.09%)  \n6     The (0.81%)    \\n\\n (0.46%)       \" (0.42%)         This (0.19%)  \n7   means (2.18%)     mod (1.79%)    will (1.73%)          was (1.38%)  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Input</th>\n      <th>Choice 1</th>\n      <th>Choice 2</th>\n      <th>Choice 3</th>\n      <th>Choice 4</th>\n      <th>Choice 5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Transformers are the</td>\n      <td>most (9.76%)</td>\n      <td>same (2.94%)</td>\n      <td>only (2.87%)</td>\n      <td>best (2.38%)</td>\n      <td>first (1.77%)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Transformers are the first</td>\n      <td>to (12.16%)</td>\n      <td>of (4.14%)</td>\n      <td>and (3.95%)</td>\n      <td>class (2.70%)</td>\n      <td>generation (1.88%)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Transformers are the first generation</td>\n      <td>of (57.15%)</td>\n      <td>to (1.53%)</td>\n      <td>, (1.07%)</td>\n      <td>in (0.93%)</td>\n      <td>. (0.68%)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Transformers are the first generation.</td>\n      <td>\\n (21.86%)</td>\n      <td>They (9.65%)</td>\n      <td>The (9.13%)</td>\n      <td>This (2.14%)</td>\n      <td>\\n\\n (2.13%)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Transformers are the first generation.\\n\\n</td>\n      <td>\\n (99.96%)</td>\n      <td>. (0.01%)</td>\n      <td>( (0.00%)</td>\n      <td>\\n\\n (0.00%)</td>\n      <td>, (0.00%)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Transformers are the first generation.\\n\\n,</td>\n      <td>the (5.36%)</td>\n      <td>and (4.23%)</td>\n      <td>which (2.85%)</td>\n      <td>a (2.17%)</td>\n      <td>\\n (2.09%)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Transformers are the first generation.\\n\\n,\\n</td>\n      <td>\\n (90.90%)</td>\n      <td>The (0.81%)</td>\n      <td>\\n\\n (0.46%)</td>\n      <td>\" (0.42%)</td>\n      <td>This (0.19%)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Transformers are the first generation.\\n\\n,\\nThis</td>\n      <td>is (22.42%)</td>\n      <td>means (2.18%)</td>\n      <td>mod (1.79%)</td>\n      <td>will (1.73%)</td>\n      <td>was (1.38%)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"max_length = 128\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length,\ndo_sample=False)\nprint(tokenizer.decode(output_greedy[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:03:51.739685Z","iopub.execute_input":"2024-12-04T09:03:51.740151Z","iopub.status.idle":"2024-12-04T09:03:53.526868Z","shell.execute_reply.started":"2024-12-04T09:03:51.740116Z","shell.execute_reply":"2024-12-04T09:03:53.525777Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\n\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 2.2 Beam Search Decoding","metadata":{}},{"cell_type":"code","source":"output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\ndo_sample=False)\nprint(tokenizer.decode(output_beam[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:03:55.764428Z","iopub.execute_input":"2024-12-04T09:03:55.765470Z","iopub.status.idle":"2024-12-04T09:03:56.883552Z","shell.execute_reply.started":"2024-12-04T09:03:55.765409Z","shell.execute_reply":"2024-12-04T09:03:56.882455Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe researchers, from the University of California, San Diego, and the University of California, Santa Cruz, found that the unicorns were able to communicate with each other in a way that was similar to that of human speech.\n\n\n\"The unicorns were able to communicate with each other in a way that was similar to that of human speech,\" said study co-lead author Dr. David J.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 2.3 Top-k Sampling","metadata":{}},{"cell_type":"code","source":"output_topk = model.generate(input_ids, max_length=max_length, do_sample=True,\ntop_k=50)\nprint(tokenizer.decode(output_topk[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:03:59.739414Z","iopub.execute_input":"2024-12-04T09:03:59.740100Z","iopub.status.idle":"2024-12-04T09:04:00.547004Z","shell.execute_reply.started":"2024-12-04T09:03:59.740062Z","shell.execute_reply":"2024-12-04T09:04:00.546004Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe group's findings were published in the Dec. 1 issue of Nature Communications, among other things.\n\n\nThe researchers describe their findings in a video released this morning by the Smithsonian Park Zoo and help researchers prepare for the next major study.\n\n\n\"Familiarity with the spoken-a language does make it easy and easy to learn other languages,\" said senior author Tom M. Hovann,\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 2.4 Top-P sampling","metadata":{}},{"cell_type":"code","source":"output_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\ntop_p=0.90)\nprint(tokenizer.decode(output_topp[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:04:02.604625Z","iopub.execute_input":"2024-12-04T09:04:02.605020Z","iopub.status.idle":"2024-12-04T09:04:03.427047Z","shell.execute_reply.started":"2024-12-04T09:04:02.604984Z","shell.execute_reply":"2024-12-04T09:04:03.426132Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe researchers, who had been studying the unicorn population and then studying their behaviour, found that the unicorns could recognize and interact with an unknown type of animal as well.\n\n'The finding is the first evidence that wild unicorns have a language. They may even have an ancestral language,' explained Professor Jochen Weich, a researcher in the Department of Natural Science and Technology at the University of\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 3.BLEU Metrice\n![image](https://miro.medium.com/v2/resize:fit:1400/1*Vm5DgYvNfl6hrqpVD3n7OA.png)","metadata":{}},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:04:21.771919Z","iopub.execute_input":"2024-12-04T09:04:21.772824Z","iopub.status.idle":"2024-12-04T09:04:32.747540Z","shell.execute_reply.started":"2024-12-04T09:04:21.772787Z","shell.execute_reply":"2024-12-04T09:04:32.746541Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:05:23.979608Z","iopub.execute_input":"2024-12-04T09:05:23.980001Z","iopub.status.idle":"2024-12-04T09:05:34.054593Z","shell.execute_reply.started":"2024-12-04T09:05:23.979967Z","shell.execute_reply":"2024-12-04T09:05:34.053433Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.0.0 sacrebleu-2.4.3\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:05:05.036084Z","iopub.execute_input":"2024-12-04T09:05:05.036981Z","iopub.status.idle":"2024-12-04T09:05:16.687444Z","shell.execute_reply.started":"2024-12-04T09:05:05.036940Z","shell.execute_reply":"2024-12-04T09:05:16.686327Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=6247ca3381e4b78a5bc36f81b7fd9fe0690aa2ebe904da7f4bf3fb2c425260d3\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from evaluate import load  # Modern API for metrics\n\n# Load the SacreBLEU metric\nbleu_metric = load(\"sacrebleu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:05:34.056449Z","iopub.execute_input":"2024-12-04T09:05:34.056825Z","iopub.status.idle":"2024-12-04T09:05:34.426601Z","shell.execute_reply.started":"2024-12-04T09:05:34.056791Z","shell.execute_reply":"2024-12-04T09:05:34.425851Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nprediction=\"the cat is on mat\" \nreference=[\"the cat is on the mat\"]\n\nbleu_metric.add(prediction = prediction, reference = reference)\nresults = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\npd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:05:37.490321Z","iopub.execute_input":"2024-12-04T09:05:37.491395Z","iopub.status.idle":"2024-12-04T09:05:37.548972Z","shell.execute_reply.started":"2024-12-04T09:05:37.491354Z","shell.execute_reply":"2024-12-04T09:05:37.547890Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                 Value\nscore                        57.893007\ncounts                    [5, 3, 2, 1]\ntotals                    [5, 4, 3, 2]\nprecisions  [100.0, 75.0, 66.67, 50.0]\nbp                            0.818731\nsys_len                              5\nref_len                              6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>score</th>\n      <td>57.893007</td>\n    </tr>\n    <tr>\n      <th>counts</th>\n      <td>[5, 3, 2, 1]</td>\n    </tr>\n    <tr>\n      <th>totals</th>\n      <td>[5, 4, 3, 2]</td>\n    </tr>\n    <tr>\n      <th>precisions</th>\n      <td>[100.0, 75.0, 66.67, 50.0]</td>\n    </tr>\n    <tr>\n      <th>bp</th>\n      <td>0.818731</td>\n    </tr>\n    <tr>\n      <th>sys_len</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>ref_len</th>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## ROUGE Evaluation ","metadata":{}},{"cell_type":"code","source":"rouge_metric = load(\"rouge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:05:41.729935Z","iopub.execute_input":"2024-12-04T09:05:41.730335Z","iopub.status.idle":"2024-12-04T09:05:42.576586Z","shell.execute_reply.started":"2024-12-04T09:05:41.730292Z","shell.execute_reply":"2024-12-04T09:05:42.575715Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdaebb41cac04a9fb16604559b244c1c"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"prediction=\"the cat is on mat\" \nreference=[\"the cat is on the mat\"]\nrecords = []\nrouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrouge_metric.add(prediction=prediction, reference=reference[0])\nscore = rouge_metric.compute()\nrouge_dict = dict((rn, score[rn]) for rn in rouge_names)\nrecords.append(rouge_dict)\npd.DataFrame.from_records(records)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:05:45.049746Z","iopub.execute_input":"2024-12-04T09:05:45.050152Z","iopub.status.idle":"2024-12-04T09:05:45.230236Z","shell.execute_reply.started":"2024-12-04T09:05:45.050116Z","shell.execute_reply":"2024-12-04T09:05:45.229105Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"     rouge1    rouge2    rougeL  rougeLsum\n0  0.909091  0.666667  0.909091   0.909091","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.909091</td>\n      <td>0.666667</td>\n      <td>0.909091</td>\n      <td>0.909091</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Text summerization ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}